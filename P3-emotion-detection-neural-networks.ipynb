{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4740_FA20_p3_im324_kl866.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epRaBvj_bapY"
      },
      "source": [
        "# Project 3: Emotion detection with Neural Networks\n",
        "## CS4740/5740 Fall 2020\n",
        "\n",
        "Names: Ikra Monjur, Komukill Loganathan\n",
        "\n",
        "Netids: im324, kl866\n",
        "\n",
        "### Project Submission Due: November 13th\n",
        "Please submit **pdf file** of this notebook on **Gradescope**, and **ipynb** on **CMS**. For instructions on generating pdf and ipynb files, please refer to project 1 instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIEkIveBVYyU"
      },
      "source": [
        "## Introduction\n",
        "In this project we will consider **neural networks**: first a Feedforward Neural Network (FFNN) and second a Recurrent Neural Network (RNN), for performing a 5-class emotion detection task.\n",
        "\n",
        "The project is divided into parts. In **Part 1**, you will be given an implementation for a FFNN and be asked to debug it in a specific way. In **Part 2**, you will then implement an RNN model for performing the same task. In **Part 3**, you will analyze these two models in two types of comparative studies and in **Part 4** you will answer questions describing what you have learned through this project. You also will be required to submit a description of libraries used, how your group divided up the work, and your feedback regarding the assignment (**Part 5**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1HSug5yWVrI"
      },
      "source": [
        "## Advice ðŸš€\n",
        "As always, the report is important! The report is where you get to show\n",
        "that you understand not only what you are doing but also why and how you are doing it. So be clear, organized and concise; avoid vagueness and excess verbiage. Spend time doing error analysis for the models. This is how you understand the advantages and drawbacks of the systems you build. The reports should read more like the papers that we have been writing critiques for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViSV4Cf86Qed"
      },
      "source": [
        "All throughout the report you may be asked to place images, plots, etc. Feel free to write code that will generate the plots for you and use those or generate them some other way and insert into the colab. To add images in your colab, these are a few possible ways to do it!\n",
        "\n",
        "1. Copy and paste the image in markdown! Yes this really does work\n",
        "\n",
        "2. Upload to google drive, get a shareable link. It will be something like:\n",
        "\n",
        "```\n",
        "https://drive.google.com/file/d/1xDrydbSbijvK2JBftUz-5ovagN2B_RWH/view?usp=sharing\n",
        "```\n",
        "We want just the id which is `1xDrydbSbijvK2JBftUz-5ovagN2B_RWH` and the link we will use is:\n",
        "\n",
        "```\n",
        "https://drive.google.com/uc?export=view&id=your_id\n",
        "```\n",
        "\n",
        "Then in markdown you'd write the following:\n",
        "\n",
        "```markdown\n",
        "![image](https://drive.google.com/uc?export=view&id=1xDrydbSbijvK2JBftUz-5ovagN2B_RWH)\n",
        "```\n",
        "\n",
        "3. Using IPython!\n",
        "```python\n",
        "from IPython.display import Image\n",
        "Image(filename=\"drive/GPU/data/iris.PNG\")\n",
        "```\n",
        "\n",
        "4. Using your connected GDrive\n",
        "```markdown\n",
        "![iris](drive/GPU/data/iris.PNG)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fyl3-8TWyR9"
      },
      "source": [
        "## Dataset\n",
        "You are given access to a set of tweets. These tweets have an associated\n",
        "emotion $y \\in Y := \\{anger, fear, joy, love, sadness\\}$. For this project, given the review text, you will\n",
        "need to predict the associated rating, y. This is sometimes called fine-grained sentiment analysis in the literature; we will simply refer to it as sentiment analysis in this project.\n",
        "\n",
        "We will minimally preprocess the reviews and handle tokenization in what we re-\n",
        "lease. For this assignment, we do not anticipate any further preprocessing to be done by you. Should you choose to do so, it would be interesting to hear about in the report (along with whether or not it helped performance), but it is not a required aspect of the assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARl1pk1PGL2Y",
        "outputId": "782fa45a-cf93-4832-d81c-d56c21737e0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = os.path.join(os.getcwd(), \"drive\", \"My Drive\", \"CS4740\", \"Project3\", \"p3-cs4740-2020fa\",\"p3_train.txt\") # replace based on your Google drive organization\n",
        "val_path = os.path.join(os.getcwd(), \"drive\", \"My Drive\", \"CS4740\", \"Project3\", \"p3-cs4740-2020fa\",\"p3_val.txt\") # replace based on your Google drive organization\n",
        "test_path = os.path.join(os.getcwd(), \"drive\", \"My Drive\", \"CS4740\", \"Project3\", \"p3-cs4740-2020fa\",\"p3_test_no_labels.txt\") # replace based on your Google drive organization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD5wwTl3W3CO"
      },
      "source": [
        "# Part 1: Feedforward Neural Network\n",
        "\n",
        "In this section, there are two main components relevant to **Part 1**.\n",
        "\n",
        "1. `Data loader`\\\n",
        "As the name suggests, this section loads the data from the dataset files and handles other preprocessing and setup. You will **not** need to change this file and should **not** change this file throughout the assignment.\n",
        "\n",
        "2. `ffnn`\\\n",
        "This contains the model and code that uses the model for **Part 1**\n",
        "\n",
        "In the `ffnn` section, you will find a Feedforward Neural Net serving as the underlying model for performing emotion detection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciCL-fBNaErA"
      },
      "source": [
        "## Part 1: Tips\n",
        "\n",
        "We do not assume you have **any** experience working with neural networks and/or debugging them. You may discover this process, while similar, is quite different from debuging in general software engineering and from debugging in other domains such as algorithms and systems.\n",
        "\n",
        "We suggest you systematically step through the code and simultanously (perhaps by physically drawing it out) describe what the computations _mean_. What you are looking for is where the code differs from what is expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGvj2WzaJNh"
      },
      "source": [
        "## Part 1: Rules\n",
        "\n",
        "For **Part 1**, you will not be able to ask any questions on Piazza and we will be unable to provide any meaningful advice in office hours. Unfortunately, this is the nature of debugging, it is unlikely anyone can give you specific advice for most problems you encounter and we have already provided general tips in the preceding section, If you absolutely must ask a question or you believe there is some kind of issue with the assignment for this part, please submit a private Piazza post and we will respond swiftly.\n",
        "\n",
        "As a reminder **communication about the assignment _between_ distinct groups is not permissed and is a violation of the Academic Integrity policy** For this assignment, we will be _extremely_ stringent about this, given that debugging is entirely pointless if someone else in a different group tells you where the error is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeC3pYiebc6r"
      },
      "source": [
        "## Import libraries and connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quIJujja-jS2"
      },
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import time\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import Dict, List, Set, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from tqdm.notebook import tqdm, trange"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU8Yq_dMbhtQ"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfVaPnSf1WU-"
      },
      "source": [
        "emotion_to_idx = {\n",
        "    \"anger\": 0,\n",
        "    \"fear\": 1,\n",
        "    \"joy\": 2,\n",
        "    \"love\": 3,\n",
        "    \"sadness\": 4\n",
        "}\n",
        "idx_to_emotion = {v: k for k, v in emotion_to_idx.items()}\n",
        "UNK = \"<UNK>\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ajMvRL3b2sz"
      },
      "source": [
        "def fetch_data(train_data_path, val_data_path, test_data_path):\n",
        "    \"\"\"fetch_data retrieves the data from a json/csv and outputs the validation\n",
        "    and training data\n",
        "\n",
        "    :param train_data_path:\n",
        "    :type train_data_path: str\n",
        "    :return: Training, validation pair where the training is a list of document, label pairs\n",
        "    :rtype: Tuple[\n",
        "        List[Tuple[List[str], int]],\n",
        "        List[Tuple[List[str], int]],\n",
        "        List[List[str]]\n",
        "    ]\n",
        "    \"\"\"\n",
        "    with open(train_data_path) as training_f:\n",
        "        training = training_f.read().split(\"\\n\")[1:-1]\n",
        "    with open(val_data_path) as valid_f:\n",
        "        validation = valid_f.read().split(\"\\n\")[1:-1]\n",
        "    with open(test_data_path) as testing_f:\n",
        "        testing = testing_f.read().split(\"\\n\")[1:-1]\n",
        "\t\n",
        "    # If needed you can shrink the training and validation data to speed up somethings but this isn't always safe to do by setting k < 10000\n",
        "    # k = #fill in\n",
        "    # training = random.shuffle(training)\n",
        "    # validation = random.shuffle(validation)\n",
        "    # training, validation = training[:k], validation[:(k // 10)]\n",
        "\n",
        "    tra = []\n",
        "    val = []\n",
        "    test = []\n",
        "    for elt in training:\n",
        "        if elt == '':\n",
        "            continue\n",
        "        txt, emotion = elt.split(\",\")\n",
        "        tra.append((txt.split(\" \"), emotion_to_idx[emotion]))\n",
        "    for elt in validation:\n",
        "        if elt == '':\n",
        "            continue\n",
        "        txt, emotion = elt.split(\",\")\n",
        "        val.append((txt.split(\" \"), emotion_to_idx[emotion]))\n",
        "    for elt in testing:\n",
        "        if elt == '':\n",
        "            continue\n",
        "        txt = elt\n",
        "        test.append(txt.split(\" \"))\n",
        "\n",
        "    return tra, val, test"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GucjLU2dXztT"
      },
      "source": [
        "def make_vocab(data):\n",
        "    \"\"\"make_vocab creates a set of vocab words that the model knows\n",
        "\n",
        "    :param data: The list of documents that is used to make the vocabulary\n",
        "    :type data: List[str]\n",
        "    :returns: A set of strings corresponding to the vocabulary\n",
        "    :rtype: Set[str]\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "    for document, _ in data:\n",
        "        for word in document:\n",
        "            vocab.add(word)\n",
        "    return vocab \n",
        "\n",
        "\n",
        "def make_indices(vocab):\n",
        "\t\"\"\"make_indices creates a 1-1 mapping of word and indices for a vocab.\n",
        "\n",
        "\t:param vocab: The strings corresponding to the vocabulary in train data.\n",
        "\t:type vocab: Set[str]\n",
        "\t:returns: A tuple containing the vocab, word2index, and index2word.\n",
        "\t\tvocab is a set of strings in the vocabulary including <UNK>.\n",
        "\t\tword2index is a dictionary mapping tokens to its index (0, ..., V-1)\n",
        "\t\tindex2word is a dictionary inverting the mapping of word2index\n",
        "\t:rtype: Tuple[\n",
        "\t\tSet[str],\n",
        "\t\tDict[str, int],\n",
        "\t\tDict[int, str],\n",
        "\t]\n",
        "\t\"\"\"\n",
        "\tvocab_list = sorted(vocab)\n",
        "\tvocab_list.append(UNK)\n",
        "\tword2index = {}\n",
        "\tindex2word = {}\n",
        "\tfor index, word in enumerate(vocab_list):\n",
        "\t\tword2index[word] = index \n",
        "\t\tindex2word[index] = word \n",
        "\tvocab.add(UNK)\n",
        "\treturn vocab, word2index, index2word \n",
        "\n",
        "\n",
        "def convert_to_vector_representation(data, word2index, test=False):\n",
        "\t\"\"\"convert_to_vector_representation converts the list of strings into a vector\n",
        "\n",
        "\t:param data: The dataset to be converted into a vectorized format\n",
        "\t:type data: Union[\n",
        "\t\tList[Tuple[List[str], int]],\n",
        "\t\tList[str],\n",
        "\t]\n",
        "\t:param word2index: A mapping of word to index\n",
        "\t:type word2index: Dict[str, int]\n",
        "\t:returns: A list of vector representations of the input or pairs of vector\n",
        "\t\trepresentations with expected output\n",
        "\t:rtype: List[Tuple[torch.Tensor, int]] or List[torch.Tensor]\n",
        "\n",
        "\tList[Tuple[List[torch.Tensor], int]] or List[List[torch.Tensor]]\n",
        "\t\"\"\"\n",
        "\tif test:\n",
        "\t\tvectorized_data = []\n",
        "\t\tfor document in data:\n",
        "\t\t\tvector = torch.zeros(len(word2index)) \n",
        "\t\t\tfor word in document:\n",
        "\t\t\t\tindex = word2index.get(word, word2index[UNK])\n",
        "\t\t\t\tvector[index] += 1\n",
        "\t\t\tvectorized_data.append(vector)\n",
        "\telse:\n",
        "\t\tvectorized_data = []\n",
        "\t\tfor document, y in data:\n",
        "\t\t\tvector = torch.zeros(len(word2index)) \n",
        "\t\t\tfor word in document:\n",
        "\t\t\t\tindex = word2index.get(word, word2index[UNK])\n",
        "\t\t\t\tvector[index] += 1\n",
        "\t\t\tvectorized_data.append((vector, y))\n",
        "\treturn vectorized_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vDZVRVG_WAp"
      },
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    \"\"\"EmotionDataset is a torch dataset to interact with the emotion data.\n",
        "\n",
        "    :param data: The vectorized dataset with input and expected output values\n",
        "    :type data: List[Tuple[List[torch.Tensor], int]]\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.X = torch.cat([X.unsqueeze(0) for X, _ in data])\n",
        "        self.y = torch.LongTensor([y for _, y in data])\n",
        "        self.len = len(data)\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"__len__ returns the number of samples in the dataset.\n",
        "\n",
        "        :returns: number of samples in dataset\n",
        "        :rtype: int\n",
        "        \"\"\"\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"__getitem__ returns the tensor, output pair for a given index\n",
        "\n",
        "        :param index: index within dataset to return\n",
        "        :type index: int\n",
        "        :returns: A tuple (x, y) where x is model input and y is our label\n",
        "        :rtype: Tuple[torch.Tensor, int]\n",
        "        \"\"\"\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "def get_data_loaders(train, val, batch_size=16):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # First we create the dataset given our train and validation lists\n",
        "    dataset = EmotionDataset(train + val)\n",
        "\n",
        "    # Then, we create a list of indices for all samples in the dataset\n",
        "    train_indices = [i for i in range(len(train))]\n",
        "    val_indices = [i for i in range(len(train), len(train) + len(val))]\n",
        "\n",
        "    for i in train_indices:\n",
        "      if (torch.any(torch.isnan(train[i][0]))):\n",
        "        print(\"NAN TRUE\")\n",
        "\n",
        "    # Now we define samplers and loaders for train and val\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    \n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_YuWdHc99TW"
      },
      "source": [
        "train, val, test = fetch_data(train_path, val_path, test_path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uS_DWhQ9CTG"
      },
      "source": [
        "vocab = make_vocab(train)\n",
        "vocab, word2index, index2word = make_indices(vocab)\n",
        "train_vectorized = convert_to_vector_representation(train, word2index)\n",
        "val_vectorized = convert_to_vector_representation(val, word2index)\n",
        "test_vectorized = convert_to_vector_representation(test, word2index, True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG7pKH07ClOv"
      },
      "source": [
        "train_loader, val_loader = get_data_loaders(train_vectorized, val_vectorized, batch_size=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7GPWuoQVzgT"
      },
      "source": [
        "# Note: Colab has 12 hour limits on GPUs, also potential inactivity may kill the notebook. Save often!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSKmtKpXbk8_"
      },
      "source": [
        "## 1.1 FFNN Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqv3EwKyaGaN"
      },
      "source": [
        "### 1.1 Task\n",
        "Assume that an onmiscient oracle has told you there are **4 fundamental errors** in the **FFNN** implementation. They may be anywhere in this section unless otherwise indicated. Your objective is to _find_ and _fix_ each of these errors and to include in the report a description of the original error along with the fix. To help your efforts, the oracle has provided you with additional information about the properties of the errors as follows:\n",
        "\n",
        "* _Correctness_ \\\n",
        "Each error causes the code to be strictly incorrect. There is absolutely no ambiguity that the errant code (or missing code) is incorrect. This means errors are not due to the code being inefficient (in run-time or in memory).\n",
        "\n",
        "* _Localized_ \\\n",
        "Each error can be judged to be erroneous by strictly looking at the code (along with your knowledge of machine learning as taught through this course). The errors therefore are not due to the model being uncompetitive in terms of performance with state-of-the-art performance for this task nor are they due to the amount of data being insufficient for this task in general.\n",
        "\n",
        "* _General_ \\\n",
        "Each error is general in nature. They will not be triggered by the model receiving a pathological input, i.e. they will not be something that is triggered specifically when NLP is referenced with negative sentiment.\n",
        "\n",
        "* _Fundamental_ \\\n",
        "Each error is a fundamental failure in terms of doing what is intended. This means that errors do not hinge on nuanced understanding of specific PyTorch functionality. This also means they will not exploit properties of the dataset in\n",
        "a subtle way that could only be realized by someone who has comprehensively studied the data.\n",
        "\n",
        "The bottom line: the errors should be fairly obvious. The oracle further reminds you that performance/accuracy of the (resulting) model should not be how you ensure you have debugged successfully. For example, if you correct some, but not all, of the errors, the remaining errors may mask the impact of your fixes. Further, performance is not guaranteed to improve by fixing any particular error. Consider the case where the training set is also employed as the test set; performance will be very high but there is something very wrong. And fixing the problem will reduce performance.\n",
        "In fixing each error, the oracle provides some further insight about the fixes:\n",
        "\n",
        "* _Minimal_ \\\n",
        "A reasonable fix for each error can be achieved in < 5 lines of code being changes. We do not require you to make fixes of 4 of fewer lines, but it should be a cause for concern if your fixes are far more elaborate\n",
        "\n",
        "* _Ill-posed_ \\\n",
        "While the errors are unambiguous, the method for fixing them is under-specified: You are free to implement any reasonable fix and all such fixes will equally recieve full credit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHbr7ltPADIu",
        "outputId": "b2889c8a-79b1-46e3-deba-37892a0ffedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "# Lambda to switch to GPU if available\n",
        "get_device = lambda : \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "get_device()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsNf3SOHcbCa"
      },
      "source": [
        "unk = '<UNK>'\n",
        "\n",
        "# Consult the PyTorch documentation for information on the functions used below:\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "\tdef __init__(self, input_dim, h, output_dim):\n",
        "\t\tsuper(FFNN, self).__init__()\n",
        "\t\tself.h = h\n",
        "\t\tself.W1 = nn.Linear(input_dim, h)\n",
        "\t\tself.activation = nn.ReLU() # The rectified linear unit; one valid choice of activation function\n",
        "\t\t## ERROR 1 occurs in the line below. The fix was to change the second h to output_dim\n",
        "\t\tself.W2 = nn.Linear(h, output_dim) \n",
        "    # The below two lines are not a source for an error\n",
        "\t\tself.softmax = nn.LogSoftmax(dim=1) # The softmax function that converts vectors into probability distributions; computes log probabilities for computational benefits\n",
        "\t\tself.loss = nn.NLLLoss() # The cross-entropy/negative log likelihood loss taught in class\n",
        "\n",
        "\tdef compute_Loss(self, predicted_vector, gold_label):\n",
        "\t\treturn self.loss(predicted_vector, gold_label)\n",
        "\n",
        "\tdef forward(self, input_vector):\n",
        "\t\t# The z_i are just there to record intermediary computations for your clarity\n",
        "\t\t##ERROR 2 occur in the line below. The fix was to apply the activation function to the W1 layer.\n",
        "\t\tz1 = self.activation(self.W1(input_vector)) \n",
        "\t\tz2 = self.W2(z1)\n",
        "\t\tpredicted_vector = self.softmax(z2)\n",
        "\t\treturn predicted_vector\n",
        "\t\n",
        "\tdef load_model(self, save_path):\n",
        "\t\tself.load_state_dict(torch.load(save_path))\n",
        "\t\n",
        "\tdef save_model(self, save_path):\n",
        "\t\ttorch.save(self.state_dict(), save_path)\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer):\n",
        "\tmodel.train()\n",
        "\ttotal = 0\n",
        "\tloss = 0\n",
        "\tacc_loss = 0\n",
        "\tcorrect = 0\n",
        "\tfor (input_batch, expected_out) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n",
        "\t\toutput = model(input_batch.to(get_device()))\n",
        "\t\ttotal += output.size()[0]\n",
        "\t\t_, predicted = torch.max(output, 1)\n",
        "\t\tcorrect += (expected_out == predicted.to(\"cpu\")).cpu().numpy().sum()\n",
        "\t\n",
        "\t\tloss = model.compute_Loss(output, expected_out.to(get_device()))\n",
        "\t\tacc_loss += loss\n",
        "\t\t##ERROR 3 was the accumulated gradients. The fix was to add the line below to zero out the gradients.\n",
        "\t\toptimizer.zero_grad() \n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t# Print accuracy\n",
        "\tacc_loss /= len(train_loader)\n",
        "\tprint(\"Training loss\", acc_loss)\n",
        "\tprint(\"Training accuracy\", correct/total)\n",
        "\n",
        "\treturn\n",
        "\n",
        "\n",
        "def evaluation(model, val_loader, optimizer):\n",
        "\tmodel.eval()\n",
        "\tloss = 0\n",
        "\tcorrect = 0\n",
        "\ttotal = 0\n",
        "\tfor (input_batch, expected_out) in tqdm(val_loader, leave=False, desc=\"Validation Batches\"):\n",
        "\t\toutput = model(input_batch.to(get_device()))\n",
        "\t\ttotal += output.size()[0]\n",
        "\t\t_, predicted = torch.max(output, 1)\n",
        "\t\tcorrect += (expected_out.to(\"cpu\") == predicted.to(\"cpu\")).cpu().numpy().sum()\n",
        "\n",
        "\t\tloss += model.compute_Loss(output, expected_out.to(get_device()))\n",
        "\tloss /= len(val_loader)\n",
        "\t# Print validation metrics\n",
        "\tprint(\"Validation loss\", loss)\n",
        "\tprint(\"Validation accuracy\", correct/total)\n",
        "\tpass\n",
        "\n",
        "def train_and_evaluate(number_of_epochs, model, train_loader, val_loader, lr=0.001):\n",
        "\toptimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\tfor epoch in trange(number_of_epochs, desc=\"Epochs\"):\n",
        "\t\t## ERROR 4 occurs in the line below. The fix was to train using the training data instead of the validation.\n",
        "\t\ttrain_epoch(model, train_loader, optimizer)\n",
        "\t\tevaluation(model, val_loader, optimizer)\n",
        "\tprint(\"\")\n",
        "\treturn"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFrrcE3CVxzX"
      },
      "source": [
        "h = 512\n",
        "model = FFNN(len(vocab), h, len(emotion_to_idx)).to(get_device())\n",
        "train_and_evaluate(2, model, train_loader, val_loader)\n",
        "model.save_model(\"ffnn_fixed.pth\") # Save our model!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZqpEPzW_lXX"
      },
      "source": [
        "# Example of how to load\n",
        "loaded_model = FFNN(len(vocab), h, len(emotion_to_idx))\n",
        "loaded_model.load_model(\"ffnn_fixed.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxW8Xgq6YbPE"
      },
      "source": [
        "## 1.2 Part 1 Report\n",
        "Please include a description of the error, a description of your fix, and a python comment indicating the fix for each of the 4 errors.\n",
        "\n",
        "### Error 1: \n",
        "Error code: `self.W2 = nn.Linear(h, h)`.\n",
        "\n",
        "The output layer (W2 layer) in the FFNN had the wrong output dimension. In the code, it was h which is the dimension of the hidden layer while it should be the output_dim given as the argument. The output dimension of the hidden layer and the output layer are not the same. \n",
        "\n",
        "We fixed this error by changing the second argument of the nn.Linear function to be the output_dim as shown below.\n",
        "\n",
        "Fixed code: `self.W2 = nn.Linear(h, output_dim)`\n",
        "\n",
        "### Error 2: \n",
        "Error code: `z1 = self.W1(input_vector)`\n",
        "\n",
        "All the hidden layers in a FFNN should have an activation function applied to them to introduce non-linearity. In the code, the hidden layer (W1 layer) did not have the activation function applied to it which was an error.\n",
        "\n",
        "To fix this error, we simply applied the activation function (which is the ReLu function in our case) to the hidden layer.\n",
        "\n",
        "Fixed code: `z1 = self.activation(self.W1(input_vector))`\n",
        "\n",
        "### Error 3: \n",
        "\n",
        "In the train_epoch function, the error was that the gradient of the optimizer (SGD in our case) was not zeroed out before each iteration. This is needed to ensure the gradient is pointing in the right direction and is not influenced by the previous accumulated gradients.\n",
        "\n",
        "In order to fix this error, we used the pytorch function zero_grad() to zero out the gradients at the beginning of each iteration.\n",
        "\n",
        "Fixed code: `optimizer.zero_grad()`\n",
        "\n",
        "### Error 4: \n",
        "Error code: `train_epoch(model, val_loader, optimizer)`\n",
        "\n",
        "The error was in train_and_evaluate function where the training was being done using the validation set. This is wrong because we should not have the same training set as the validation set since we will always get a high validation accuracy. We will not know if the model overfits if the train and validation sets are the same.\n",
        "\n",
        "To fix this, we used the training dataset to train the model.\n",
        "\n",
        "Fixed code: `train_epoch(model, train_loader, optimizer)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D2uTXRmbn5Q"
      },
      "source": [
        "# Part 2: Recurrent Neural Network\n",
        "Recurrent neural networks have been the workhorse of NLP for a number of years. A fundamental reason for this success is they can inherently deal with _variable_ length sequences. This is axiomatically important for natural language; words are formed from a variable number of characters, sentences from a variable number of words, paragraphs from a variable number of sentences, and so forth. This differs from a field like Computer Vision where images are (generally) of a fixed size.\n",
        "<br></br>\n",
        "This is also very different scenario than that of the classifiers we have studied (e.g.Naive Bayes, Perceptron Learning, Feedforward Neural Networks), which take in a\n",
        "fixed-length vector.\n",
        "<br></br>\n",
        "To clarify this, we can think of the _types_ of the mathematical functions described by a FFNN and an RNN. What is pivotal in what follows is that k need not be constant\n",
        "across examples.\n",
        "\n",
        "$\\textbf{FFNN.}$ \\\n",
        "$Input: \\vec{x} \\in \\mathcal{R}^d$ \\\n",
        "$Model\\text{ }Output: \\vec{z} \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$\n",
        "$Final\\text{ }Output: \\vec{y} \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$ \\\n",
        "$\\vec{y}$ satisfies the contraint of being a probability distribution, ie $\\underset{i \\in \\mid \\mathcal{Y} \\mid}{\\sum} \\vec{y}[i] = 1$ and $\\underset{i \\in \\mid \\mathcal{y} \\mid}{min} \\text{ }\\vec{y}[i] \\leq 1$, which is achieved via _Softmax_ applied to $\\vec{z}$.\n",
        "<br></br>\n",
        "$\\textbf{RNN.}$ \\\n",
        "$Input: \\vec{x}_1,\\vec{x}_2, \\dots, \\vec{x}_k; \\vec{x}_i \\in \\mathcal{R}^d$ \\\n",
        "$Model\\text{ }Output: \\vec{z}_1,\\vec{z}_2, \\dots, \\vec{z}_k; \\vec{z}_i \\in \\mathcal{R}^{h}$\n",
        "$Final\\text{ }Output: \\vec{y} \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$ \\\n",
        "$\\vec{y}$ satisfies the contraint of being a probability distribution, ie $\\underset{i \\in \\mid \\mathcal{Y} \\mid}{\\sum} \\vec{y}[i] = 1$ and $\\underset{i \\in \\mid \\mathcal{y} \\mid}{min} \\text{ }\\vec{y}[i] \\geq 0$, which is achieved by the process described later in this report and as you have seen in class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4n8IVFtflPQ"
      },
      "source": [
        "Intuitively, an RNN takes in a sequence of vectors and computes a new vector corresponding to each vector in the original sequence. It achieves this by processing the input sequence one vector at a time to (a) compute an updated representation of the entire sequence (which is then re-used when processing the next vector in the input sequence), and (b) produce an output for the current position. The vector computed in (a) therefore not only contains information about the current input vector but also about the previous input vectors. Hence, $\\vec{z}_j$ is computed after having observed $\\vec{x}_1, \\dots, \\vec{x}_j$. As such, a simple observation is we can treat the last vector computed by the RNN, ie $\\vec{z}_k$ as a representation of the entire sequence. Accordingly, we can use this as the input to a single-layer linear classifier to compute a yector $\\vec{y}$ as we will need for classification.\n",
        "\n",
        "$$\\vec{y} = Softmax(W\\vec{z}_k); W\\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid \\times h}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFYJdd9kYhXq"
      },
      "source": [
        "## Part 2: Rules\n",
        "**Part 2** requires implementing a rudimentary RNN in PyTorch for text classification. Countless blog posts, internet tutorials and other implementations available publicly (and privately) do precisely this. In fact, almost every student in [Cornell NLP](https://nlp.cornell.edu/people/) likely has some code for doing this on their Github. You **cannot** use any such code (though you may use anything you find in course notes or course texts) irrespective of whether you cite it or do not.\n",
        "\n",
        "Submissions will be passed through the MOSS system, which is a sophisticated system for detecting plagiarism in code and is robust in the sense that it tries to find alignments in the underlying semantics of the code and not just the surface level syntax. Similarly, the course staff are also quite astute with respect to programming neural models for NLP and we will strenuously look at your code. We flagged multiple groups for this last year, so we strongly suggest you resist any such temptation (if the Academic Integrity policy alone is insufficient at dissuading you)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS1lQBsWWNLc"
      },
      "source": [
        "## 2.1 RNN Implementation\n",
        "\n",
        "Similar to **Part 1**, we have the previous `Data loader` section and the new `RNN` component. We don't envision that it will be useful to modify the `Data loader`. We have included some stubs to help give you a place to start for the RNN.\n",
        "\n",
        "Additionally, we remind you that Part 1 furnishes a near-functional implementation of a similar neural model for the same task. If you successfully do Part 1 correctly, it will be wholely functional. Using it as a template for Part 2 is both prudent and suggested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69LxTPLWEPEG"
      },
      "source": [
        "# get max document length for vector dimension\n",
        "max_len = 0\n",
        "for document,_ in train:\n",
        "  if max_len < len(document):\n",
        "    max_len = len(document)\n",
        "\n",
        "#pre-trained GloVe word embeddings used is of dimension 50\n",
        "word_vec_dim = 50\n",
        "\n",
        "def rnn_preprocessing(data, test=False, max_len=max_len):\n",
        "    \"\"\"rnn_preprocessing creates a list of tensors that have word vectors for each document in data.\n",
        "\n",
        "    :param data: data to be preprocessed\n",
        "    :type data: list of string lists, or list of 2-tuples with elements, string list and int\n",
        "    :param test: if pre-processing is done for test data\n",
        "    :type test: boolean\n",
        "    :param max_len: maximum length a document can be (to create word vectors tensor)\n",
        "    :type max_len: int\n",
        "    \"\"\"\n",
        "    # Do some preprocessing similar to convert_to_vector_representation\n",
        "    # For the RNN, remember that instead of a single vector per training\n",
        "    # example, you will have a sequence of vectors where each vector\n",
        "    # represents some information about a specific token.\n",
        "    docs = []\n",
        "    if test:\n",
        "      max_len = 0\n",
        "      for document in data:\n",
        "        docs.append(document)\n",
        "        if max_len < len(document):\n",
        "          max_len = len(document)\n",
        "    else:\n",
        "      for document,_ in data:\n",
        "        docs.append(document)\n",
        "\n",
        "    # Use Pretrained GloVe from Twitter Data with 50 dimensions to extract word embeddings \n",
        "    glove_path = os.path.join(os.getcwd(), \"drive\", \"My Drive\", \"CS4740\", \"Project3\", \"p3-cs4740-2020fa\",\"glove.twitter.27B.50d.txt\")\n",
        "    embeddings_dict = {}\n",
        "    with open(glove_path, 'r') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            embeddings_dict[word] = vector\n",
        "\n",
        "    vector = []\n",
        "    if test:\n",
        "      for document in data:\n",
        "        doc_vector = torch.zeros([max_len, word_vec_dim], dtype=torch.float32) \n",
        "        for i in range(len(document)):\n",
        "          word = document[i]\n",
        "          if word in embeddings_dict:\n",
        "            word_vec = embeddings_dict[word]\n",
        "          doc_vector[i] = torch.from_numpy(word_vec)\n",
        "        vector.append(doc_vector)\n",
        "    else:\n",
        "      for document,y in data:\n",
        "        doc_vector = torch.zeros([max_len, word_vec_dim], dtype=torch.float32) \n",
        "        for i in range(len(document)):\n",
        "          word = document[i]\n",
        "          if word in embeddings_dict:\n",
        "            word_vec = embeddings_dict[word]\n",
        "          doc_vector[i] = torch.from_numpy(word_vec)\n",
        "        vector.append((doc_vector, y))\n",
        "    return vector"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AJRcXxRnbMy"
      },
      "source": [
        "# pre-process train and validation data\n",
        "train_vectorized_rnn = rnn_preprocessing(train)\n",
        "val_vectorized_rnn = rnn_preprocessing(val)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xux1gCmR8cMe"
      },
      "source": [
        "train_loader_rnn, val_loader_rnn = get_data_loaders(train_vectorized_rnn, val_vectorized_rnn, batch_size=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgwzPVKMQ3lV"
      },
      "source": [
        "from torch import autograd\n",
        "class RNN(nn.Module):\n",
        "\tdef __init__(self, input_dim, h, output_dim): # Add relevant parameters\n",
        "\t\tsuper(RNN, self).__init__()\n",
        "\t\tself.h = h\n",
        "\t\tself.W = nn.Linear(input_dim, h)\n",
        "\t\tself.V = nn.Linear(h, output_dim)\n",
        "\t\tself.U = nn.Linear(h, h)\n",
        "\t\tself.activation = nn.LeakyReLU()\t\n",
        "\t\t# using LeakyReLU to ensure gradients to ensure small vector values are not affecting hidden vectors \n",
        "\t\t# Ensure parameters are initialized to small values, see PyTorch documentation for guidance\n",
        "\t\tself.softmax = nn.LogSoftmax(dim=1)\n",
        "\t\tself.loss = nn.NLLLoss()\n",
        "\n",
        "\tdef compute_Loss(self, predicted_vector, gold_label):\n",
        "\t\treturn self.loss(predicted_vector, gold_label)\n",
        "\n",
        "\tdef forward(self, inputs):\n",
        "\t# begin code\n",
        "\t\th_vec = torch.zeros(self.h).to(get_device())\n",
        "\t\tzero_inp = torch.zeros((1,word_vec_dim)).to(get_device())\n",
        "\t\tfor i in range(0, inputs.size()[1]):\n",
        "\t\t\tinp = torch.reshape(inputs[0][i], (1,word_vec_dim))\n",
        "\t \t\t# stop training on document if no more words\n",
        "\t\t\tif torch.all(torch.eq(inp, zero_inp)): \n",
        "\t\t\t\tbreak\n",
        "\t\t\th_vec = self.activation(self.U(h_vec) + self.W(inp))\n",
        "\t\t\t\n",
        "\t\ty = self.V(h_vec)\n",
        "\t\t# remember to include the predicted unnormalized scores which should be normalized into a (log) probability distribution\n",
        "\t\t# end code\n",
        "\t\treturn self.softmax(y)\n",
        "\n",
        "\tdef load_model(self, save_path):\n",
        "\t\tself.load_state_dict(torch.load(save_path))\n",
        "\t\n",
        "\tdef save_model(self, save_path):\n",
        "\t\ttorch.save(self.state_dict(), save_path)\n",
        "\t\n",
        "def train_epoch_rnn(model, train_loader, optimizer):\n",
        "\tmodel.train()\n",
        "\ttotal = 0\n",
        "\tloss = 0\n",
        "\tcorrect = 0\n",
        "\tacc_loss = 0\n",
        "\tfor (input_batch, expected_out) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n",
        "\t\toutput = model(input_batch.to(get_device()))\n",
        "\t\ttotal += output.size()[0]\n",
        "\t\t_, predicted = torch.max(output, 1)\n",
        "\t\tcorrect += (expected_out == predicted.to(\"cpu\")).cpu().numpy().sum()\n",
        "\t\tloss = model.compute_Loss(output, expected_out.to(get_device()))\n",
        "\t\tacc_loss += loss\n",
        "\t\toptimizer.zero_grad() \n",
        "\t\tloss.backward()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\tacc_loss /= len(train_loader)\n",
        "\t# Print accuracy\n",
        "\tprint(\"Training accuracy\", correct/total)\n",
        "\tprint(\"Training loss\", acc_loss.item())\n",
        "\treturn acc_loss\n",
        "\n",
        "def evaluation_rnn(model, val_loader, optimizer):\n",
        "\tmodel.eval()\n",
        "\tloss = 0\n",
        "\tcorrect = 0\n",
        "\ttotal = 0\n",
        "\tfor (input_batch, expected_out) in tqdm(val_loader, leave=False, desc=\"Validation Batches\"):\n",
        "\t\toutput = model(input_batch.to(get_device()))\n",
        "\t\ttotal += output.size()[0]\n",
        "\t\t_, predicted = torch.max(output, 1)\n",
        "\t\tcorrect += (expected_out.to(\"cpu\") == predicted.to(\"cpu\")).cpu().numpy().sum()\n",
        "\t\tloss += model.compute_Loss(output, expected_out.to(get_device()))\n",
        "\tloss /= len(val_loader)\n",
        "\t# Print validation metrics\n",
        "\tprint(\"Validaiton loss\", loss.item())\n",
        "\tprint(\"Validation accuracy\", correct/total)\n",
        "\treturn loss\n",
        "\t\n",
        "def train_and_evaluate_rnn(number_of_epochs, model, train_loader, val_loader, lr=0.001):\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\tfor epoch in trange(number_of_epochs, desc=\"Epochs\"):\n",
        "\t\tprint(\"Epoch\", epoch+1)\n",
        "\t\ttr_loss = train_epoch_rnn(model, train_loader, optimizer)\n",
        "\t\tval_loss = evaluation_rnn(model, val_loader, optimizer)\n",
        "\t\tscheduler.step()\n",
        "\treturn\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8awXTFZgf3d"
      },
      "source": [
        "h = 250\n",
        "model = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())\n",
        "train_and_evaluate_rnn(10, model, train_loader_rnn, val_loader_rnn)\n",
        "model.save_model(\"rnn.pth\") # Save our model!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9MNOOPXRrLr"
      },
      "source": [
        "# Example of how to load\n",
        "loaded_model = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())\n",
        "loaded_model.load_model(\"rnn.pth\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UbsqRZtWsp_"
      },
      "source": [
        "## 2.2 Part 2 Report\n",
        "For Part 2, your report should have a description of each major step of implementing the RNN accompanied by the associated code-snippet. Each step should have an explanation for why you decided to do something (when one could reasonably do the same step in a different way); your justification will not be based on empirical results in this section but should relate to something we said in class, something mentioned in any of the course texts, or some other source (i.e. literature in NLP or official PyTorch documentation). **Unjustified, vague, and/or under-substantiated explanations will not receive credit.**\n",
        "\n",
        "Things to include:\n",
        "\n",
        "1. _Representation_ \\\n",
        "Each $\\vec{x}_i$ needs to be produced in some way and should correspond to word $i$ in the text. This is different from the text classification approaches we have studied previously (BoW for example) where the entire document is represented with a single vector. Where and how is this being done for the RNN?\n",
        "\n",
        "2. _Initialization_ \\\n",
        "There will be weights that you update in training the RNN. Where and how are these initialized?\n",
        "\n",
        "3. _Training_ \\\n",
        "You are given the entire training set of N examples. How do you make use of this training set? How does the model modify its weights in training (this likely entails somewhere where gradients are computed and somehwere else where these gradients are used to update the model)?\n",
        "\n",
        "4. _Model_ \\\n",
        "This is the core model code, ie. where and how you apply the RNN to the $\\vec{x}_i$\n",
        "\n",
        "5. _Linear Classifier_ \\\n",
        "Given the outputs of the RNN, how do you consume these to actually compute $\\vec{y}$?\n",
        "\n",
        "6. _Stopping_ \\\n",
        "How does your training procedure terminate?\n",
        "\n",
        "7. _Hyperparameters_ \\\n",
        "To run your model, you must fix some hyperparameters, such as $h$ (the hidden dimensionality of the $\\vec{z}_i$ referenced above). Be sure to exhaustively describe these hyperparameters and why you set them as you did ( this almost certainly will require some brief exploration: we suggest the course text by Yoav Goldberg as well as possibly the PyTorch official documentation). Be sure to accurately cite either source.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA519cwBuhIW"
      },
      "source": [
        "### 2.2.1 Representation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqvbTja2Fmv"
      },
      "source": [
        "Our input data is created to be of a specific form in the rnn_preprocessing function. We represent our input data in the following way:\n",
        "\n",
        "Each word in each of the documents is represented by a vector. So, each document is represented by multiple vectors. And, the size of the vectors for each document is set to be the maximum document length in order to have uniform dimension across all document vectors. This avoids dimension mismatch errors for further computations. Smaller documents are padded with zeros, which does not affect further calculations.\n",
        "\n",
        "To create the word vectors for each document, we used pretrained GloVe with Twitter data with 50 dimension word embeddings. We chose a dimension of 50 since it is enough to get the embedding of each word since tweets are generally not too long. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MwU9A8jumDH"
      },
      "source": [
        "\n",
        "### 2.2.2 Initialization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB2JyiWM2GX0"
      },
      "source": [
        "We use 3 weights in our RNN and these are initialized in the constructor of the RNN class. We initialized our weights W, V, and U using PyTorch's Linear transformation function.\n",
        "\n",
        "` self.W = nn.Linear(input_dim, h)`\n",
        "\n",
        "` self.V = nn.Linear(h, output_dim)`\n",
        "\n",
        "` self.U = nn.Linear(h, h) `\n",
        "\n",
        "Weight matrix W is initialized with a dimension of input_dim by h, where input_dim is the size of the word vector and h is the size of the hidden layer. \n",
        "\n",
        "Weight matrix U is initialized with a dimension of h by h, where h is the size of the hidden layer. U is then multiplied with the previous hidden layer, to get the context of the previous parts of the sequence. Adding this product to the product of W and input, and applying the activation function gives the current hidden layer. The activation function we use is the LeakyReLU function. This function is similar to ReLu function but takes care of the dying ReLU problem.\n",
        "\n",
        "Weight matrix V is initialized with a dimension of h by output_dim, where h is the size of hidden layer and output_dim is the size of the output, which is the number of emotions we have. \n",
        "\n",
        "We used pytorch Linear function to ensure that the weights are initialized to be small random values as mentioned in the pytorch documentation. This is important because random values break symmetry in the weights to make sure that the hidden layer values are not the same for the first round of calculations. Small values are important so that the slope of the gradient does not change too slowly which would make the learning take a long time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHzl4RR6vZZO"
      },
      "source": [
        "### 2.2.3 Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx_wSNoq2KyT"
      },
      "source": [
        "We train our model using the train_epoch_rnn function. In here, we are looping through each of our inputs and calculating the predicted value using our RNN model. The output from the model returns a vector of probabilities for each of the emotions and to get the predicted emotion, we take the emotion corresponding to the maximum probability in the vector. The loss is calculated using cross entropy loss function. The backward function is called on this loss which calculates the new gradient. This gradient is then used to update the weights when the step function is called on the optimizer, which is the Adam optimizer from pytorch in our case. We tried training our model with SGD but the loss was decreasing at a very slow rate and did not improve much over multiple epochs. Adam optimizer, on the other hand, had a good pattern of reduction in loss and the model learned faster. Before we calculate the new gradient using `loss.backward()`, we zero out the gradients in order to avoid accumulation of the gradients from previous training iterations. We are also using a scheduler in our training process which changes the learning rate after 5 epochs. The learning rate is decreased by a factor of 10 making the model learn slower so it can learn things it missed previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCBvcHgRvn8x"
      },
      "source": [
        "### 2.2.4 Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzUweVa72MoU"
      },
      "source": [
        "We apply our RNN to the inputs in forward function in RNN class.\n",
        "```\n",
        "h_vec = torch.zeros(self.h).to(get_device())\n",
        "\t\tzero_inp = torch.zeros((1,word_vec_dim)).to(get_device())\n",
        "\t\tfor i in range(0, inputs.size()[1]):\n",
        "\t\t\tinp = torch.reshape(inputs[0][i], (1,word_vec_dim))\n",
        "\t\t\tif torch.all(torch.eq(inp, zero_inp)):\n",
        "\t\t\t\tbreak\n",
        "\t\t\th_vec = self.activation(self.U(h_vec) + self.W(inp))\n",
        "\t\ty = self.V(h_vec)\n",
        "return self.softmax(y)\n",
        "```\n",
        "We initialize our h vector to be a zero vector for the first time step. We then loop through the word vectors of the input. For each word vector, we calculate the current h vector by adding the product of weight matrix W and the input word vector to the product of weight matrix U and the previous h vector, and applying the activation function on this sum. The activation function used is LeakyReLU function in order to take care of the dying ReLU problem. We made all the input vectors the same size (the size of the maximum tweet) during our preprocessing, but many of the inputs are actually smaller than that making them contain many zero word vectors at the end of the actual input. Using these zero vectors would affect our model and we only want to take into account the last actual word of the document so we added a check to exit the for-loop when we encounter a word vector that is filled with zeros. \n",
        "\n",
        "Outside the loop, we calculate our output by multiplying the last hidden layer vector and the weight matrix V and applying the softmax function on this product to normalize the probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgoleuX2vrLz"
      },
      "source": [
        "### 2.2.5 Linear Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjia6Ge32O4v"
      },
      "source": [
        "We compute the final prediction vector by multiplying the last hidden layer by the weight matrix V in RNN forward function. Then, softmax (log softmax in our case) is applied on this product in order to normalize it. RNN returns this normalized prediction vector ($\\vec{y}$) for each input. Lastly, while training and evaluating, we are getting one emotion label per document by taking the emotion corresponding to the maximum probability from the prediction vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEfJKPWlvvPO"
      },
      "source": [
        "### 2.2.6 Stopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb4Tc3Fe2Shr"
      },
      "source": [
        "Our training procedure terminates after the given number of epochs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpT2Ntgmv0RD"
      },
      "source": [
        "### 2.2.7 Hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdSHK0HF2ipj"
      },
      "source": [
        "**Number of hidden dimension:**  We chose our hidden dimension to be 250  given that we have about 10,000 inputs and the maximum length of one document is 66 words where one word vector is of dimension 50. Given these input information, we need to choose a hidden dimension value that is not too large nor too small. The number of hidden dimension corresponds to the amount of information being passed into each of the layers in the network. Since our dataset consists of tweets which are not too large, a hidden dimension smaller than the text size would mean not enough information is being propagated. We chose 250 after some experimentation and with 250 as the hidden dimension, the network seemed very stable and seemed to learn properly.\n",
        "\n",
        "\n",
        "**Epochs:** For the number of epochs, we chose it to be 10. After trying out multiple epochs (we tried 5 epochs, 10 epochs, and 15 epochs). We noticed that 5 epochs was too little since when we changed the number of epochs to be 10, both the training loss and the validation loss decreased. However, when we changed the number of epochs to be 15, it seemed like there was definitely overfitting occuring since the training loss decreased but the validation loss stayed around the same (sometimes increased negligibly). Thus, after such experimentation, we chose our number of epochs to be 10 so the model has enough time to learn well without overfitting. \n",
        "\n",
        "**Learning Rate:**\n",
        "For the learning rate, we initialized it to be 0.001. According to the text by Yoav Goldberg, a big learning rate means that the model will have trouble converging to a proper solution while if the learning rate is too small, the model will be learning very slowly. We are also using a scheduler which decreases learning rate after 5 epochs since after 5 epochs we noticed that the model starts to not learn as much (the validation loss starts to plateau)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJqQTVxdZjQ3"
      },
      "source": [
        "# Part 3: Analysis\n",
        "From **Part 1** and **Part 2**, you will have two different models in hand for performing the same emotion detection task. In **Part 3**, you will conduct a comprehensive analysis of these models, focusing on two comparative settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szhOtAUuheQU"
      },
      "source": [
        "## Part 3 Note\n",
        "You will be required to submit the code used in finding these results on CMSX. This code should be legible and we will consult it if we find issues in the results. It is worth noting that in **Part 1** and **Part 2**, we primarily are considering the correctness of the code-snippets in the report. If your model is flawed in a way that isnâ€™t exposed by those snippets, this will likely surface in your results for **Part 3**. We will deduct points for correctness in this section to reflect this and we will try to localize where the error is (or think it is, if it is opaque from your code). That said, we will be lenient about absolute performance (within reason) in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPT5RutAZzaU"
      },
      "source": [
        "## 3.1: Across-Model Comparison\n",
        "In this section, you will report results detailing the comparison of the two models. Specifically, we will consider the issue of _fair comparison_<sup>5</sup>, which is a fundamental notion in NLP and ML research and practice. In particular, given model $A$, it is likely the case we can make a model $B$ that is computationally more complex and, hence, more costly and achieves superior performance. However, this makes for an unfair comparison. For our purposes, we want to study how the FFNN and RNN compare when we try to control for hyperparameters and other configurable values being of similar computational cost<sup>6</sup>. That said, it is impossible to have identical configurations as these are different models, i.e. the RNN simply has hyperparameters for which there are no analogues in the FFNN.\n",
        "\n",
        "\n",
        "In the report you will need to begin by describing 3 pairs of configurations, with each pair being comprised of a FFNN configuration and a RNN configuration that constitute a _fair comparison_. You will need to argue for why the two parts of each pair are a fair comparison. Across the pairs, you should try different types of configurations (e.g. trying to resolve like questions of the form: _Does the FFNN perform better or worse when the hidden dimensionality is small as opposed to when it is large?_) and justify what you are trying to study by having the results across the pairs.\n",
        "\n",
        "\n",
        "Next, you will report the quantitative accuracy of the 6 resulting models. You will\n",
        "analyze these results and then move on to a more descriptive analysis.\n",
        "\n",
        "The descriptive analysis can take one of two forms<sup>7</sup>:\n",
        "\n",
        "1. _Nuanced quantitative analysis_ \\\n",
        "If you choose this option, you will need to further break down the quantitative statistics you reported initially. We provide some initial strategies to prime you for what you should think about in doing this: one possible starting point is to consider: if model $X$ achieves greater accuracy than model $Y$, to what extent is $X$ getting everything correct that $Y$ gets correct? Alternatively, how is model performance affected if you measure performance on a specific strata/subset of the reviews?\n",
        "\n",
        "2. _Nuanced qualitative analysis_ \\\n",
        "If you choose this option, you will need to select individual examples and try to explain or reason about why one model may be getting them right whereas the other isnâ€™t. Are there any examples that all 6 models get right or wrong and, if so, can you hypothesize a reason why this occurs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ygOuN5ekcx1K",
        "outputId": "7717d2f2-0499-4ed9-902d-83616e15dde0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "#@markdown â €\n",
        "display(HTML('''<hr><p style=\"font-family:verdana; font-size:90%;\">\n",
        "5. This term takes on different meanings in different settings. Here we simply mean that we are trying to\n",
        "compare different models while controlling for similar â€œcomplexityâ€/computational cost. <br></br>\n",
        "\n",
        "6. We have not taught you how to do this rigorously and the theory for doing this is still underdeveloped. We only expect a reasonable attempt. <br></br>\n",
        "\n",
        "7. This is the minimal requirement, if you provide other, more elaborate, analyses, we certainly welcome this.\n",
        "</p>'''))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<hr><p style=\"font-family:verdana; font-size:90%;\">\n",
              "5. This term takes on different meanings in different settings. Here we simply mean that we are trying to\n",
              "compare different models while controlling for similar â€œcomplexityâ€/computational cost. <br></br>\n",
              "\n",
              "6. We have not taught you how to do this rigorously and the theory for doing this is still underdeveloped. We only expect a reasonable attempt. <br></br>\n",
              "\n",
              "7. This is the minimal requirement, if you provide other, more elaborate, analyses, we certainly welcome this.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Zqj0etww3C"
      },
      "source": [
        "### 3.1.1 Configuration 1\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZDaU7yBVpyo"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\"\"\"\n",
        "eval_preds evaluates the predictions of [model] for a [data_vectorized].\n",
        "Prints various evaluation metrics for each emotion.\n",
        "\n",
        "model: the trained NN model \n",
        "data_vectorized: vectorized data and its associated labels\n",
        "\"\"\"\n",
        "def eval_preds(model, data_vectorized):\n",
        "  correct = 0\n",
        "  tp_dict = defaultdict(int)  # if an emotion is predicted right, it is true positive (tp)\n",
        "  fp_dict = defaultdict(int)  # if the prediction is wrong, it is false positive for predicted emotion (fp)\n",
        "  emo_dict = defaultdict(int) # to store the count of emotions in training set\n",
        "  preds = []\n",
        "  total=len(data_vectorized)\n",
        "  for idx, (input_vector, expected) in tqdm(enumerate(data_vectorized), total=len(data_vectorized)):\n",
        "    output = model(torch.Tensor(input_vector).unsqueeze(0).to(get_device())).cpu()\n",
        "    _, pred = torch.max(output, 1)\n",
        "    pred = int(pred)\n",
        "    correct += (expected == pred)\n",
        "    preds.append((idx, pred, expected))\n",
        "    if pred == expected:\n",
        "      tp_dict[pred] += 1\n",
        "    else:\n",
        "      fp_dict[pred] += 1\n",
        "    emo_dict[expected] += 1\n",
        "  \n",
        "  print(\"Accuracy\", correct/total)\n",
        "  print(\"Emotion counts in training set\", emo_dict)\n",
        "  print(\"True positive counts:\", tp_dict)\n",
        "  print(\"False positive counts:\", fp_dict)\n",
        "  print(\"\")\n",
        "\n",
        "  for emo in emotion_to_idx:\n",
        "    idx = emotion_to_idx[emo]\n",
        "    # true positive rate\n",
        "    tp_rate = tp_dict[idx] / emo_dict[idx]\n",
        "    # predictive power\n",
        "    pred_power = tp_dict[idx] / (fp_dict[idx] + tp_dict[idx] + 1)\n",
        "    print(\"Emotion:\", idx, emo)\n",
        "    print(\"True positive rate:\", tp_rate)\n",
        "    print(\"Predictive power:\", pred_power)\n",
        "    print(\"\")\n",
        "  return preds"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MxvtuTPfaWi"
      },
      "source": [
        "# constants for both model\n",
        "h = 512\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "ffnn_config_1 = FFNN(len(vocab), h, len(emotion_to_idx)).to(get_device())\n",
        "rnn_config_1 = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lfUjGpYUfn5"
      },
      "source": [
        "#training FFNN\n",
        "train_and_evaluate(epochs, ffnn_config_1, train_loader, val_loader, lr=lr)\n",
        "ffnn_config_1.save_model(\"ffnn_config_1.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8tUlfcT3eU"
      },
      "source": [
        "#training RNN\n",
        "train_and_evaluate_rnn(epochs, rnn_config_1, train_loader_rnn, val_loader_rnn, lr=lr)\n",
        "rnn_config_1.save_model(\"rnn_config_1.pth\") # Save our model!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpx_tb6oc3Lw"
      },
      "source": [
        "#get first 10 predictions using ffnn and rnn\n",
        "print(\"--- FFNN Predictions\")\n",
        "ffnn_preds = eval_preds(ffnn_config_1, val_vectorized)\n",
        "print(\"\\n--- RNN Predictions\")\n",
        "rnn_preds = eval_preds(rnn_config_1, val_vectorized_rnn)\n",
        "\n",
        "print(\"--- Some examples of predictions and expected output ---\\n\")\n",
        "for i in range(10):\n",
        "  print(\"INPUT:\", \" \".join(val[i][0]))\n",
        "  print(\"ffnn pred\", ffnn_preds[i][1])\n",
        "  print(\"rnn pred\", rnn_preds[i][1])\n",
        "  print(\"correct\", ffnn_preds[i][2])\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKTJLa2hyjaz"
      },
      "source": [
        "### 3.1.1 Report\n",
        "Describe configurations, report the results, and then perform a nuanced analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM1ZNhqK2yx-"
      },
      "source": [
        "For the first and second configurations, we control the number of epochs and learning rate while varying the dimension of hidden layer to study how dimension of hidden layer influences the performance of the neural networks.\n",
        "\n",
        "For the first configuration, we have 512 hidden dimensions, a learning rate of 0.001, and 10 epochs for both FFNN and RNN models. This is a fair comparison because when we control the number of epochs, the computations done on both models amounts to about the same cost as both models need to be trained with 10 epochs. And since we are using a batch size of 1, the number of computations and iterations done on training are the same. Controlling the learning rate ensures that both models learn at the same rate, converging from a similar starting point.\n",
        "\n",
        "## Result\n",
        "When this configuration is used to train FFNN and RNN models, we found that the FFNN learns slightly better than RNN. FFNN has a final training accuracy of 0.99 and loss of 0.0307, which is slightly better than RNN that has training accuracy of 0.95 and loss of 0.26. With a hidden dimension of 512, both models learn well gradually, with the training loss decreasing in a stable manner over the 10 epochs.\n",
        "\n",
        "The trained models also predict the validation set pretty well, where both FFNN and RNN models have 0.89 as the validation accuracy. However, the loss of validation is higher in RNN with 0.78 whereas FFNN has a loss of 0.34 on validation after 10 epochs. These loses are low which indicates that 512 is a good hidden dimension, such that both models FFNN and RNN do not overfit, yet still perform well on the validation set. More on how hidden dimension affects model performance will be explored in the next configuration, as we will have a comparison point there.\n",
        "\n",
        "## Nuanced analysis - Qualitative Analysis\n",
        "\n",
        "Firstly, we'd like to explore how the FFNN model learns and predicts in a qualitative sense. We see, from the evaluation metrics printed above, that FFNN predicts anger emotion the best, followed by fear and joy. Some examples that FFNN gets right are:\n",
        "\n",
        "*   `i didn t know that i would feel so completely exhausted` as sadness\n",
        "*   `i couldn t help but feel slightly skeptical and apprehensive as i realized the tough task funes was taking on that night` as fear\n",
        "\n",
        "We see that these sentences are predicted correctly because our FFNN model picks on the meaning of words like \"exhausted\" which is more related to sadness and \"skeptical\" and \"apprehensive\" which are more likely to be fear.\n",
        "\n",
        "RNN, on the other hand, predicts anger the best as well, followed by sadness. Some correct examples:\n",
        "\n",
        "* `i feel excluded and worthless my connection to everyone summarily cut off` as sadness\n",
        "\n",
        "* `i shouldnt feel threatened by that` as fear\n",
        "\n",
        "These make sense as the RNN preprocessed input would give the meanings of words like \"threatened\" to be similar to fear and \"excluded\" and \"worthless\" to be sadness. The RNN model picks up these patterns and learns to predict as such.\n",
        "\n",
        "Some examples that FFNN gets right but RNN does not for this configuration are:\n",
        "\n",
        "* `i know she feels helpless but that kiss that cuddle the hug every morning and the love you every night` is sadness but RNN predicts as fear. Here, it could be that RNN picks up words like \"helpless\" to get a prediction of fear, which might be weighted more by the RNN model than the rest of the sentence. FFNN correctly gets the semantic of the sentence that the sentence has a sad tone.\n",
        "\n",
        "* `i just act how i feel im becoming what ive always hated` is anger but RNN predicts as fear. RNN is again picking up words like \"hated\" to mean something sad but FFNN captures the entire semantic of the sentence, perhaps remembering words like \"feel\" and \"becoming\", which are words that one generally does not say in an angry statement.\n",
        "\n",
        "Some examples that RNN gets right but FFNN does not:\n",
        "\n",
        "* `i enjoy going to churches acquired there feeling is always so peaceful and tranquil thats why ive had a wish to visit pochayiv monastery and without comments it was really worthy` is joy but FFNN predicts as sadness. RNN correctly picks up the meaning of words like \"enjoy\", whereas FFNN is not performing very well in this with the appearance of possible negative words like \"without\", \"wish\" which one tends to say more when they are sad.\n",
        "\n",
        "* `i guess we would naturally feel a sense of loneliness even the people who said unkind things to you might be missed` is anger but FFNN predicts as sadness. RNN here understands the semntic of the sentence, that the sentence has a passive aggressive tone. FFNN might interpret words like \"loneliness\", \"unkind\", \"missed\" to mean sadness.\n",
        "\n",
        "An example that both models get right:\n",
        "\n",
        "* `i get why she is concerned because i have been pretty honest about feeling shitty about all of it` as sadness.\n",
        "\n",
        "This shows that both models are able to capture words like \"concerned\", \"shitty\" to be sadness as there are no words that can have ambiguous meaning when they appear in a different context. And, hence, both models predict well in this case.\n",
        "\n",
        "For how hidden dimension influences individual predictions, we can look at the next configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdNiBJPrRvg4"
      },
      "source": [
        "### 3.1.2 Configuration 2\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkrm_tmzF-MJ"
      },
      "source": [
        "#constants for both model\n",
        "h = 50\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "ffnn_config_2 = FFNN(len(vocab), h, len(emotion_to_idx)).to(get_device())\n",
        "rnn_config_2 = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbNomVMbj-AZ"
      },
      "source": [
        "#training FFNN\n",
        "train_and_evaluate(epochs, ffnn_config_2, train_loader, val_loader, lr=lr)\n",
        "ffnn_config_2.save_model(\"ffnn_config_2.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q3x5aFqkBBi"
      },
      "source": [
        "#training RNN\n",
        "train_and_evaluate_rnn(epochs, rnn_config_2, train_loader_rnn, val_loader_rnn, lr=lr)\n",
        "rnn_config_2.save_model(\"rnn_config_2.pth\") # Save our model!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o96XNym_kMYg"
      },
      "source": [
        "#get first 10 predictions using ffnn and rnn\n",
        "print(\"--- FFNN Predictions\")\n",
        "ffnn_preds = eval_preds(ffnn_config_2, val_vectorized)\n",
        "print(\"\\n--- RNN Predictions\")\n",
        "rnn_preds = eval_preds(rnn_config_2, val_vectorized_rnn)\n",
        "\n",
        "print(\"--- Some examples of predictions and expected output ---\\n\")\n",
        "for i in range(10):\n",
        "  print(\"INPUT:\", \" \".join(val[i][0]))\n",
        "  print(\"ffnn pred\", ffnn_preds[i][1])\n",
        "  print(\"rnn pred\", rnn_preds[i][1])\n",
        "  print(\"correct\", ffnn_preds[i][2])\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR1kOmYIyqdV"
      },
      "source": [
        "### 3.1.2 Report\n",
        "Describe configurations, report the results, and then perform a nuanced analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM5sh56n2-DC"
      },
      "source": [
        "For the second configuration, we have 50 hidden dimensions, a learning rate of 0.001, and 10 epochs for both FFNN and RNN models. This is a fair comparison because, as described in the previous report, when we control the number of epochs, the computations done on both models amounts to about the same cost as both models need to be trained with 10 epochs. And since we are using a batch size of 1, the number of computations and iterations done on training are the same. Controlling the learning rate ensures that both models learn at the same rate, converging from a similar starting point.\n",
        "\n",
        "## Result\n",
        "When this configuration is used to train FFNN and RNN models, we found that the FFNN still learns better than RNN, where FFNN has a training accuracy of 0.98 and loss of 0.04, and RNN has a training accuracy of 0.85 and loss of 0.76. Both models perform fairly well on the validation set as well where FFNN has a validation accuracy of 0.89 and loss of 0.33, and RNN has a validation accuracy of 0.81 and loss of 1.009.\n",
        "\n",
        "However, the FFNN and RNN models with a higher hidden dimension (h=512) in the previous configuration perform better than both of these model. The FFNN model with h=512 performs better than both the FFNN and RNN in this configuration as the loss of the previous FFNN is lower than the models in this configuration, whereas the RNN model performs better than the RNN in this configuration but not better than the FFNN model in this configuration.\n",
        "\n",
        "## Nuanced Analysis - Qualitative Analysis\n",
        "\n",
        "The FFNN model in this configuration predicts anger and sadness the best, whereas the RNN model predicts anger the best. The overall predictive power for both of these models have dropped from the previous configuration with a higher hidden dimension. This shows that a higher hidden dimension produces a higher performing model. A higher hidden dimension makes a better performing model because more information or pattern is captured from the training data when there is a bigger dimension of hidden layer.\n",
        "\n",
        "However, we want to make sure that the model does not overfit the training data, and a really, really big hidden dimension could make the model overfit and hence, it would not produce a good model. At the same time, we don't want a small hidden dimension either because as we see in this configuration, h=50 performs worse than h=512. A good dimension of hidden layer needs to be picked.\n",
        "\n",
        "As explored in the previous section, the variations between just two FFNN models and two RNN models of different configurations, respectively, are not the most interesting part that we want to analyze. Like any improvement on a model, the FFNN with higher hidden dimension (config 1) will predict better than the FFNN with lower hidden dimension (config 2). The same goes to the RNN models.  For example,\n",
        "\n",
        "* `i guess we would naturally feel a sense of loneliness even the people who said unkind things to you might be missed` is anger. RNN of config 1 predicts this correctly, but RNN of config 2 predicts this as sadness. FFNN of both configs predict this as sadness. This shows that RNN with a smaller dimension loses some information on preserving the semantics of passive aggressive tone in this document, which the RNN with higher dimension preserved.\n",
        "\n",
        "* `i know she feels helpless but that kiss that cuddle the hug every morning and the love you every night` is sadness. RNN of both cofigs predict this incorrectly as anger. FFNN of config 1 predicts correctly but FFNN of config 2 predicts this incorrectly as anger as FFNN with a smaller hidden dimension loses information on longing semantics of the document which makes it to have sadness as the emotion. The FFNN with higher dimension has this information preserved.\n",
        "\n",
        "Across these 2 pairs of configurations, some examples are always predicted correctly by all 4 models. Some exmaples of those are:\n",
        "\n",
        "* `i shouldnt feel threatened by that` is fear. \n",
        "* `i feel excluded and worthless my connection to everyone summarily cut off` is sadness.\n",
        "\n",
        "These sort of documents are predicted correctly by all of these 4 models regardless of the hidden dimension configured because these sentences are simple, and have only words that are neutral or emotionally direct words like \"exhausted\" or \"excluded\" and \"worthless\". There is no ambiguity which requires a well-learned, nicely fit model to get correct predictions.\n",
        "\n",
        "Some examples where RNN has predicted incorrectly across configurations:\n",
        "\n",
        "* `i just act how i feel im becoming what ive always hated` is anger but is predicted as sadness in both config 1 and config 2. \n",
        "* `i know she feels helpless but that kiss that cuddle the hug every morning and the love you every night` is sadness but is predicted as fear in both configs.\n",
        "\n",
        "This is because RNN captures words like \"hated\" and \"helpless\" to mean emotions that we generically mean as opposed to taking the semantic of the entire sentences. The context of these words is lost in RNN no matter how the hidden dimension changes in these two configurations. \n",
        "\n",
        "An example that FFNN consistently gets incorrect across these two configuration is:\n",
        "* `i guess we would naturally feel a sense of loneliness even the people who said unkind things to you might be missed` which is anger but is predicted as sadness. Here, the FFNN is not capturing the passive aggressive context of the document in either configuration. But, the RNN model captures this information when a larger hidden dimension is used.\n",
        "\n",
        "Hence, we can say that a higher hidden dimension is better than a smaller hidden dimension to create a better performing model, for both FFNN and RNN architecture. However, we want to refrain from overfitting the model, so a really large hidden dimension should be avoided. \n",
        "\n",
        "Next, we will study how learning rate affects the performance of a model for both FFNN and RNN. The result of configuration 2 will be used to compare with the result of configuration 3 for this study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WnWktoVVkSe"
      },
      "source": [
        "### 3.1.3 Configuration 3\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt2jRwgBF9-e"
      },
      "source": [
        "#constants for both model\n",
        "h = 50\n",
        "epochs = 10\n",
        "lr = 0.01 # increased learning rate\n",
        "ffnn_config_3 = FFNN(len(vocab), h, len(emotion_to_idx)).to(get_device())\n",
        "rnn_config_3 = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3J8KmaESqZ2"
      },
      "source": [
        "#training FFNN\n",
        "train_and_evaluate(epochs, ffnn_config_3, train_loader, val_loader, lr=lr)\n",
        "ffnn_config_3.save_model(\"ffnn_config_3.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlgBbqQXSzCa"
      },
      "source": [
        "#training RNN\n",
        "train_and_evaluate_rnn(epochs, rnn_config_3, train_loader_rnn, val_loader_rnn, lr=lr)\n",
        "rnn_config_3.save_model(\"rnn_config_3.pth\") # Save our model!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6zN6EF6S4Zr"
      },
      "source": [
        "#get first 10 predictions using ffnn and rnn\n",
        "print(\"--- FFNN Predictions\")\n",
        "ffnn_preds = eval_preds(ffnn_config_3, val_vectorized)\n",
        "print(\"\\n--- RNN Predictions\")\n",
        "rnn_preds = eval_preds(rnn_config_3, val_vectorized_rnn)\n",
        "\n",
        "print(\"--- Some examples of predictions and expected output ---\\n\")\n",
        "for i in range(10):\n",
        "  print(\"INPUT:\", \" \".join(val[i][0]))\n",
        "  print(\"ffnn pred\", ffnn_preds[i][1])\n",
        "  print(\"rnn pred\", rnn_preds[i][1])\n",
        "  print(\"correct\", ffnn_preds[i][2])\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXbIgduYy3Jf"
      },
      "source": [
        "### 3.1.3 Report\n",
        "Describe configurations, report the results, and then perform a nuanced analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29_f6bu63BGl"
      },
      "source": [
        "In the third configuration, we want to study the importance of choosing a good learning rate. How does learning rate influence the performance of FFNN and RNN? We will then compare this pair of configuration with Config 2 that also has a hidden dimension of 50 and 10 epochs, but 0.001 as it's learning rate for both FFNN and RNN. \n",
        "\n",
        "So, for the third configuration, we have 50 hidden dimensions, a learning rate of 0.01, and 10 epochs for both FFNN and RNN models. This is a fair comparison because when we control the number of epochs, the computations done on both models amounts to about the same cost as both models need to be trained with 10 epochs, as said in the previous two parts. Keeping the hidden dimension the same across FFNN and RNN ensures that the same amount of information is preserved in each forward and backward propagations of the models from epoch to epoch. \n",
        "\n",
        "## Result\n",
        "When this configuration is used to train FFNN and RNN models, we found that both of the trained models perform much worse than the previous two configurations. For a fair comparison, we can compare this configuration with the previous configuration that has h=50, number of epochs=10 and learning rate=0.001, where the learning rate is the variable studied in this round. \n",
        "\n",
        "We see that with a higher learning rate, both models perform worse than those with a lower learning rate. The FFNN model has a final training accuracy of 0.9 and loss of 0.31. The validation accuracy is 0.77 and the loss is 0.74. This performance is lower than that of config 2. The RNN model, however, is especially really bad at learning as it has a final training accuracy of 0.21 and loss of NaN, which indicates that the model has an explosive gradient problem. As a result the validation accuracy is 0.21 and loss is NaN as well.Such scenario is possible because the learning rate determines how fast the gradient of the model optimizer advances, and hence how fast and how optimally the model learns. With a large learning rate, the optimizer can miss the optimal point and jump to the wrong direction and can cause an explosion in loss as the model gets worse. For the case of RNN, the scheduler that we use only reduces the learning rate after 5 epochs, there is an texplosion that happens in the 4th epoch, before the scheduler reduces the given learning rate by a factor of 10. From there on, the model gets worse and stays as such as the weight vectors have really small numbers now. The FFNN model, however, is not as bad as the RNN but still has worse performance than the FFNN and RNN of the previous configuration (with smaller learning rate), which says that the FFNN does not learn as well with a higher learning rate as well.\n",
        "\n",
        "## Nuanced Analysis - Qualitative Analysis\n",
        "FFNN performs pretty badly compared to the previous configuration which had a lower learning rate. This shows that a larger learning rate affects performance of the trained model negatively as a lot of information will be stepped over when the model advances at a fast rate. Some exmaples of predictions that did not work well:\n",
        "\n",
        "* `i just act how i feel im becoming what ive always hated` is anger. The FFNN model predicts this to be sadness. This is because information about the nuanced semantic of the document is not captured by the fast advancing optimizer in the model. \n",
        "\n",
        "* `i guess we would naturally feel a sense of loneliness even the people who said unkind things to you might be missed` is anger but FFNN has been consistently getting this incorrect by predicting it to be sadness in all three configurations. This is an example that cannot be captured by the studied hidden dimensions and learning rates.\n",
        "\n",
        "With a really bad RNN model in this configuration, the RNN model always predicts anger as the emotion for all documents. This means that no information or pattern is captured by the model from the training data after 10 epochs, and all documents are predicted as anger as that is the first emotion in the emotion dictionary and probability vector of the output of RNN.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Of6kWAcfgde"
      },
      "source": [
        "## Part 3.2: Within-model comparison\n",
        "To complement **Part 3.1: Across-Model Comparison**, in **Part 3.2: Within-Model Comparison**, you will need to study what happens when you change parameters within a model. To limit your workload, you need only do this for the RNN; and you may use at most one RNN model from the prior section.\n",
        "\n",
        "In the prior section, we discussed _fair comparison_. Anothr aspect of rigorous experimentation in NLP (and other domains) is the _ablation study_. In this, we _ablate_ or remove aspects of a more complex model, making it less complex, to evaluate whether each aspect was neccessary. To be concrete, for this part, you should train 4 variants of the RNN model and describe them as we do below:\n",
        "\n",
        "1. Baseline model\n",
        "2. Baseline model made more complex by modification $A$ (e.g. changing the hidden dimensionality from $h$ to $2h$).\n",
        "3. Baseline model made more complex by modification $B$ (where $B$ is an entirely distinct/different update from $A$).\n",
        "4. Baseline model with both modificatons $A$ and $B$ applied.\n",
        "\n",
        "Under the framing of an ablation study, you woud describe this as beginning with model 4 and then ablating (i.e. removing) each of the two modifications, in turn; and then removing both to see if they were genuinely neccessary for the performance you observe.\n",
        "\n",
        "Once you describe each of the four models, report the quantitative accuracy as in the previous section. Conclude by performing the **opposite** nuanced analysis from the one you did in the previous section (i.e. if in **Part 3.1: Across-Model Comparison** you did _Nuanced quanitative analysis_, for **Part 3.2: Within-Model Comparison** perform a _Nuanced qualitative analysis_ and vice versa)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TULtij8lzQpt"
      },
      "source": [
        "### 3.2.1 Configuration 1\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBoo8VMeD103"
      },
      "source": [
        "# hyperparameters from fixed rnn model\n",
        "h = 250\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "baseline_rnn = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())\n",
        "train_and_evaluate_rnn(epochs, baseline_rnn, train_loader_rnn, val_loader_rnn, lr=lr)\n",
        "baseline_rnn.save_model(\"baseline_rnn.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUltoKKNFGvC"
      },
      "source": [
        "print(\"Baseline RNN\")\n",
        "baseline_preds = eval_preds(baseline_rnn, val_vectorized_rnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILuGuDpfzT7A"
      },
      "source": [
        "### 3.2.1 Report\n",
        "Describe variants in the ablation style described, report the results, and then perform a nuanced analysis of the opposite type as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kSddhDL3Eiq"
      },
      "source": [
        "Report written in section 3.2 below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Q4LpZOzxa6"
      },
      "source": [
        "### 3.2.2 Configuration 2\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPnS20oPhiV0"
      },
      "source": [
        "# train and evaluate with early stopping\n",
        "def train_and_evaluate_rnn_es(number_of_epochs, model, train_loader, val_loader, lr=0.001):\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\tprev_loss = 0\n",
        "\tfor epoch in trange(number_of_epochs, desc=\"Epochs\"):\n",
        "\t\ttr_loss = train_epoch_rnn(model, train_loader, optimizer)\n",
        "\t\tval_loss = evaluation_rnn(model, val_loader, optimizer)\n",
        "    #early stopping when validation loss starts becoming larger than training loss\n",
        "\t\tif tr_loss <= val_loss:\n",
        "\t\t\treturn\n",
        "\t\tscheduler.step()\n",
        "\treturn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSE9bbSnGI2A"
      },
      "source": [
        "# early stopping\n",
        "h = 250\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "mod_a_rnn = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())\n",
        "train_and_evaluate_rnn_es(epochs, mod_a_rnn, train_loader_rnn, val_loader_rnn, lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyefln_VFKob"
      },
      "source": [
        "print(\"Early Stop RNN\")\n",
        "mod_a_preds = eval_preds(mod_a_rnn, val_vectorized_rnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GdWOwZGz3OQ"
      },
      "source": [
        "### 3.2.2 Report\n",
        "Describe variants in the ablation style described, report the results, and then perform a nuanced analysis of the opposite type as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyqXcmap3Hm8"
      },
      "source": [
        "Report written in section 3.2 below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVG-Pqa9z60-"
      },
      "source": [
        "### 3.2.3 Configuration 3\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WsI_BPpGJ5r"
      },
      "source": [
        "# reverse inputs\n",
        "h = 250\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "mod_b_rnn = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())\n",
        "train_vectorized_rnn_rev = train_vectorized_rnn[::-1]\n",
        "val_vectorized_rnn_rev = val_vectorized_rnn[::-1]\n",
        "train_loader_rnn_rev, val_loader_rnn_rev = get_data_loaders(train_vectorized_rnn_rev, val_vectorized_rnn_rev, batch_size=1)\n",
        "train_and_evaluate_rnn(epochs, mod_b_rnn, train_loader_rnn_rev, val_loader_rnn_rev, lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYdWb8-gFM98"
      },
      "source": [
        "print(\"Reversed Input RNN\")\n",
        "mod_b_preds = eval_preds(mod_b_rnn, val_vectorized_rnn_rev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9mNRTAhz8gf"
      },
      "source": [
        "### 3.2.3 Report\n",
        "Describe variants in the ablation style described, report the results, and then perform a nuanced analysis of the opposite type as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJSSzAkv3KWl"
      },
      "source": [
        "Report written in section 3.2 below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfukYWxcz-Za"
      },
      "source": [
        "### 3.2.4 Configuration 4\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbwLS9lEGLJU"
      },
      "source": [
        "# early stopping and reverse input\n",
        "h = 250\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "both_mod_rnn = RNN(word_vec_dim, h, len(emotion_to_idx)).to(get_device())\n",
        "train_and_evaluate_rnn_es(epochs, both_mod_rnn, train_loader_rnn_rev, val_loader_rnn_rev, lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlef0jAfFO7K"
      },
      "source": [
        "print(\"Early Stopping + Reversed Input RNN\")\n",
        "both_mod_preds = eval_preds(both_mod_rnn, val_vectorized_rnn_rev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CiPPaO-0Dmy"
      },
      "source": [
        "### 3.2.4 Report\n",
        "Describe variants in the ablation style described, report the results, and then perform a nuanced analysis of the opposite type as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81c2YMpY3MYH"
      },
      "source": [
        "Report written in section 3.2 below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZMMnKTcFg1D"
      },
      "source": [
        "### 3.2 Report\n",
        "Describe variants in the ablation style described, report the results, and then perform a nuanced analysis of the opposite type as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QDS_ALnFaz-"
      },
      "source": [
        "**Modification 1:** The first modification we chose to use for our model was including early stopping. Since if we are training our model for too many epochs, it can cause overfitting where the training accuracy increases while the validatio accuracy stays the same or decreases. Thus, introducing early stopping can prevent that and improve the model. So, we wanted to see if our number of epochs was good or early stopping actually helps with the accuracy.\n",
        "\n",
        "**Modification 2:** The second modification we chose to do was reversing the inputs when feeding them into the RNN. We chose this modification since for many NLP applications, feeding the input backward into the RNN tends to improve the result (our reasoning is mentioned in 4.2). We wanted to see if for our sentiment analysis task, if reversing the input actually makes much of a difference.\n",
        "\n",
        "**Ablation Study with Nuanced Quantitative Analysis:** In order to perform a quantitative analysis, we are using accuracy, true positive rate and predictive power across each of the emotion classes. A higher true positive rate means for that specific emotion, the model did well. A higher predictive power suggests that when the model predicts that emotion, the predictions are correct more often than not.\n",
        "\n",
        "Below is the table containing the **accuracy** of each model:\n",
        "\n",
        "| Model | Accuracy | \n",
        "| --- | --- | \n",
        "| Baseline RNN | 0.875  |  \n",
        "| RNN with early stopping| 0.791 |\n",
        "| RNN with reversed input | 0.877 |\n",
        "| RNN with early stopping and reversed input| 0.806|\n",
        "\n",
        "Below is the table containing the **true positive rate** for each of the models for each emotion:\n",
        "\n",
        "| Emotion Class | Baseline RNN  | RNN with early stopping | RNN with reversed inputs | RNN with both |\n",
        "| --- | --- | --- | --- | --- |\n",
        "|Anger | 0.89 | 0.83 | 0.89 | 0.86 |\n",
        "| Fear | 0.84 | 0.67 | 0.87 | 0.64 |\n",
        "| Joy | 0.87 | 0.83 | 0.88 | 0.89 |\n",
        "| Love| 0.88 | 0.78 | 0.88 | 0.80 |\n",
        "| Sadness | 0.89 | 0.81 | 0.87 | 0.80 |\n",
        "\n",
        "\n",
        "Below is the table containing the **predictive power** for each of the models for each emotion:\n",
        "\n",
        "| Emotion Class | Baseline RNN  | RNN with early stopping | RNN with reversed inputs | RNN with both |\n",
        "| --- | --- | --- | --- | --- |\n",
        "|Anger | 0.88 | 0.80 | 0.88 | 0.79 |\n",
        "| Fear | 0.86 | 0.90 | 0.84 | 0.91 |\n",
        "| Joy | 0.86 | 0.74 | 0.85 | 0.76 |\n",
        "| Love| 0.86 | 0.75 | 0.88 | 0.82 |\n",
        "| Sadness | 0.90 | 0.79 | 0.92 | 0.81 |\n",
        "\n",
        "\n",
        "Model 4 (with both modifications) is the baseline model with both early stopping and reversed input. When we ablate one of these modifications we have our model 2 with only the early stopping and model 3 with only reversed inputs. Our model 1 is simply without any of these modifications (i.e the baseline model). For all these models the hyperparameters (number of hidden dimensions, learning rate, and number of epochs) were kept the same in order to make sure we only get the effect of the modification we are making. \n",
        "\n",
        "*Model 4 (both) vs Model 2 (early stopping):*\n",
        "\n",
        "In terms of accuracy, model 2 does slightly worse than model 4. Thus, it means that ablating the reversed inputs makes the model less accurate meaning it is an important feature of the model. When looking across all the emotion classes, model 4 does much better in predicting texts that are labeled as joy with a true positive rate of 0.89 while model 2 has a true positive rate of 0.83. However, for fear, model 2 does slightly better than model 4 but they both have a true positive rate below 0.7. In terms of predicitve power, both models have similar quantities. The largest difference is for texts labeled as love. The predictive power is much higher for model 4 compared to model 2 which suggests that when model 4 predicts a text to be the label love, it is more often accurate than when model 2 predicts a text to be of label love. \n",
        "\n",
        "*Model 4 (both) vs Model 3 (reversed inputs):*\n",
        "\n",
        "Model 3 has a an accuracy of 87.7% while model 4 has an accuracy of 80.6%. The 7% difference is a big difference. In terms of true positive rate across all the emotions, model 3 does much better than model 4. The biggest difference is for texts that are labeled with fear, the difference is true positive rate is 0.23 which is quite a lot. This means that when we are stopping early, the model has not learned enough to detect texts are labeled as fear. When looking at the predictive power for the emotion fear, model 4 has a higher predictive power of 0.91 while model 3 has a predictive power of 0.84. This implies that whenever model 4 predicts a text as fear, it gets it right more often than model 3 but since we also know that the true positive rate is low for model 4 for fear, it must be the case that model 4 does not classify texts as fear too often compared to model 2. Besides fear, model 3 has a higher predictive power for all the other emotions compared to model 4.\n",
        "\n",
        "*Model 4 (both) vs Baseline:* \n",
        "\n",
        "The baseline model is much more accurate than the model 4 with both modification. The baseline accuracy is 87.5% while model 4 accuracy is 80.6%. Similar to model 3, baseline model has much higher true positive rate for fear compared to model 4. The difference is 0.2. However, model 4 has a higher predictive power than baseline for fear. This, again, means that model 4 does not classify texts as fear as often as the baseline, hence having a lower true positive rate but higher predictive power. For the other emotions, the baseline has a higher true positive rate meaning it classifies them correctly more often than model 4. Baseline also has a higher predictive power for the other emotions besides fear.\n",
        "\n",
        "*Model 2 (early stopping) vs Baseline:* \n",
        "\n",
        "Model 2 accuracy is 79.1% while the baseline has an accuracy of 87.5%. The difference in accuracy indicates that removing early stopping makes the model much more accurate. Across all the emotions, model 2 has a lower true positive rate compares to the baseline. Specifically, for both fear and joy, model 2 has a much lower true positive rate meaning that model 2 has a harder time classifying texts that are of label fear or joy correctly compared to the other emotions. Model 2 has lower predictive power for all the emotions except fear compared to the baseline. This suggests that when model 2 predicts a text to be an emotion, it is predicting them wrong more often than the baseline model. For fear, the predictive power is higher while the true positive rate is lower, which implies that model 2 does not predict a text to be of label fear as often. So, when it does, it gets them correct resulting in a higher predictive power.\n",
        "\n",
        "*Model 3 (reversed inputs) vs Baseline:*\n",
        "\n",
        "The baseline model and model 3 have very similar accuracy, differing in only 0.2%. Looking across all emotions, they both have high true positive rates. For fear and joy, however, model 3 seems to be doing slightly better compared to the baseline model. For sadness, model 3 does slightly worse than the baseline model. The predictive powers across all the emotions are also very high for both of the models. However, the baseline model predictive powers seem more stable across all the emotions than model 3. These quantities suggest that ablating reversed inputs does not make too much of a difference.\n",
        "\n",
        "Overall, it seems to be the case that removing early stopping makes the model much better, especially for the emotion fear. When the model is stopped early, it does not seem to have acquired enough information to classify a text as fear. Since our baseline models only included 10 epochs, it is most likely not the case that it is overfitting but if the validation loss is slightly higher than the training loss after an iteration, the training will be stopped. Choosing a different early stopping condition might have been better but since we only have 10 epochs to begin with, early stopping does not seem to be necessary. Removing the Reversal of the inputs makes the model slightly worse but when looking across all the statistics, it seems to indicate that it does not make too much of a difference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmdw-pyh4Lj"
      },
      "source": [
        "# Part 4: Questions\n",
        "In **Part 4**, you will need to answer the three questions below. We expect answers tobe to-the-point; answers that are vague, meandering, or imprecise **will receive fewer points** than a precise but partially correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juQW_iF_ETtg"
      },
      "source": [
        "## 4.1 Q1\n",
        "Earlier in the course, we studied models that make use of _Markov_ assumptions. Recurrent neural networks do not make any such assumption. That said, RNNs are known to struggle with long-distance dependencies. What is a fundamental reason for why this is the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIZAmdtoVpBK"
      },
      "source": [
        "RNNs struggle with long-distance dependencies because when we backpropagate through time to calculate weight updates for each of the previous time steps, we take the gradient of a later layer and multiply it with the previous layer to get the gradient of the previous layer. So, as we keep multiplying across layers like this, until we reach the initial layer, we now get a really small gradient for the initial layer because of the repeated multiplications of smaller and smaller numbers across layers. This means that the weights for the previous layers are not updated properly which diminishes the dependencies on the earlier information. This is also known as the vanishing gradient problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CmtdmOWEboE"
      },
      "source": [
        "## 4.2 Q2\n",
        "In applying RNNs to tasks in NLP, we have discovered that (at least for tasks in English) feeding a sentence into an RNN backwards (i.e. inputting the sequence of vectors corresponding to ($course$, $great$, $a$, $is$, $NLP$) instead of ($NLP$, $is$, $a$, $great$, $course$)) tends to improve performance. Why might this be the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i69IrRd4Vpdh"
      },
      "source": [
        "For a language like English, where the sentences take the form of subject-verb-object, the subject and the verb of the sentence give us more information about tenses, gender, singularity/plurality of the subject, as well as other part-of-speech details than the object does. So, when we input such sentences in the original order into RNN, we tend to lose these information acquired at the beginning of the sentence as the model advances, especially if the input sentence is long. So, if we input a reversed sentence, we will be able to preserve the essential information we get at the beginning of the sentence because we process them later, and these information will have higher weights when calculating the output in the model. Hence, reversed inputs tend to improve the performance of RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB6y4__iEk6_"
      },
      "source": [
        "## 4.3 Q3\n",
        "In using RNNs and word embeddings for NLP tasks, we are no longer required to engineer specific features that are useful for the task; the model discovers them automatically. Stated differently, it seems that neural models tend to discover better features than human researchers can directly specify. This comes at the cost of systems having to consume tremendous amounts of data to learn these kinds of patterns from the data. Beyond concerns of dataset size (and the computational resources required to process and train using this data as well as the further environmental harm that results from this process), why might we disfavor RNN models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOdh784UVp9f"
      },
      "source": [
        "We might disfavor RNN models because learning only through a large amount of dataset will make the model be biased which we do not want. This is because datasets from the real world reflects real world biases and the model will pick these patterns. For example, gender-role stereotypes can be generally reflected in large datasets, where these data usually come from historically collected information. If the RNN learns features by itself only through the dataset given, it will pick up these stereotypes creating a gender-biased model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD0pgD0Jh84T"
      },
      "source": [
        "# Part 5: Miscellaneous\n",
        "List the libraries you used and sources you referenced and cited (labelled with the section in which you referred to them). Include a description of how your group split\n",
        "up the work. Include brief feedback on this asignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snL2qqbR3SbN"
      },
      "source": [
        "**References and Citation**\n",
        "\n",
        "We used a pretrained GloVe word embeddings dataset from [Stanford NLP GloVe project site](https://nlp.stanford.edu/projects/glove/) to pre-process the input for RNN training. We specifically used the Twitter data with vector dimensions of 50, which is available for public use. (used in section 2.1)\n",
        "\n",
        "We used pytorch to implement our RNN model. We used the nn module in pytorch to initialize our weight matrices. (used in section 2.1)\n",
        "\n",
        "We refered to Yoav Goldberg, Neural Network Methods for Natural Language Processing (Chapter 5), to understand how to work with the hyperparameters of the neural network. (used in section 2.2.7)\n",
        "\n",
        "**Work**\n",
        "\n",
        "We met up and worked on the project through pair-programming. \n",
        "\n",
        "**Assignment Feedback**\n",
        "\n",
        "This project was very interesting and taught us a lot about neural networks and how to use pytorch properly to implement them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VeQ8uBwiR-a"
      },
      "source": [
        "**Each section must be clearly labelled, complete, and the corresponding pages should be correctly assigned to the corresponding Gradescope rubric item.** If you follow these steps for each of the 4 components requested, you are guaranteed full credit for this section. Otherwise, you will receive no credit for this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqPxplOuZ876"
      },
      "source": [
        "# Part 6: Kaggle Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5wMEwKX_4Hs"
      },
      "source": [
        "# Create Kaggle submission function\n",
        "kaggle_model = loaded_model\n",
        "#rnn_document_preprocessor = lambda x: rnn_preprocessor(x, True) # This is for your RNN\n",
        "file_name = \"submission.csv\"\n",
        "ffnn_document_preprocessor = lambda x: convert_to_vector_representation(x, word2index, True)\n",
        "rnn_document_preprocessor = lambda x: rnn_preprocessing(x, True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFc6rb5y8ScE"
      },
      "source": [
        "def generate_submission(filename, model, document_preprocessor, test):\n",
        "    test_vectorized = document_preprocessor(test)\n",
        "    with Path(file_name).open(\"w\") as fp:\n",
        "        fp.write(\"Id,Predicted\\n\")\n",
        "        for idx, input_vector in tqdm(enumerate(test_vectorized), total=len(test_vectorized)):\n",
        "            output = model(torch.Tensor(input_vector).unsqueeze(0).to(get_device())).cpu()#.squeeze(0)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            fp.write(f\"{idx},{int(pred)}\\n\")\n",
        "    return"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu2n9Kn9-c1U"
      },
      "source": [
        "generate_submission(file_name, kaggle_model, rnn_document_preprocessor, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nu4zL4nWnGB"
      },
      "source": [
        "# Live running demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "4qO4HFGI92-5"
      },
      "source": [
        "#@title Emotion Detection\n",
        "#@markdown Enter a sentence to see the emotion\n",
        "input_string = \"I am so joyful!\" #@param {type:\"string\"}\n",
        "model_type = \"ffnn_config_1\" #@param [\"baseline_ffnn\", \"baseline_rnn\", \"mod_a_rnn\", \"mod_b_rnn\", \"both_mods_rnn\", \"ffnn_config_1\", \"rnn_config_1\", \"ffnn_config_2\", \"rnn_config_2\", \"ffnn_config_3\", \"rnn_config_3\"]\n",
        "from IPython.display import HTML\n",
        "\n",
        "output = \"\"\n",
        "\n",
        "# BAD THING TO DO BELOW!!\n",
        "model_used = globals()[model_type]\n",
        "\n",
        "with torch.no_grad():\n",
        "    if \"ffnn\" in model_type:\n",
        "        vec_in = ffnn_document_preprocessor([[input_string]])[0]\n",
        "        model_output = model_used(torch.Tensor(vec_in).unsqueeze(0)).cpu().squeeze(0)\n",
        "    else:\n",
        "        # RUN MODEL\n",
        "        vec_in = rnn_document_preprocessor([[input_string]])[0]\n",
        "        model_output = model_used(torch.Tensor(vec_in).unsqueeze(0)).cpu().squeeze(0)\n",
        "    #print(torch.cat([torch.Tensor(z).unsqueeze(0) for z in model_inputs]).unsqueeze(0).shape)\n",
        "    #model_output = model_used(torch.cat([torch.Tensor(z).unsqueeze(0) for z in model_inputs]).unsqueeze(0))\n",
        "    #print(model_output.shape)\n",
        "predicted = torch.argmax(model_output)\n",
        "# MAP BACK TO EMOTION\n",
        "# print(int(predicted))\n",
        "emotion = idx_to_emotion[int(predicted)]\n",
        "\n",
        "# Generate nice display\n",
        "output += '<p style=\"font-family:verdana; font-size:110%;\">'\n",
        "output += \" Input sequence: \"+input_string+\"</p>\"\n",
        "output += '<p style=\"font-family:verdana; font-size:110%;\">'\n",
        "output += f\" Emotion detected: {emotion}</p><hr>\"\n",
        "output = \"<h3>Results:</h3>\" + output\n",
        "\n",
        "display(HTML(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWn1vEU-giEz"
      },
      "source": [
        "%%capture\n",
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIKLd4hAgjtW"
      },
      "source": [
        "%%capture\n",
        "# the red text is a placeholder! Change it to your directory structure!\n",
        "!cp 'drive/My Drive/Colab Notebooks/4740_FA20_p3_im324_kl866.ipynb' ./ "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMt3aLGKgosi",
        "outputId": "564f46d1-bbbf-4f6d-8dae-083c67a9daf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# the red text is a placeholder! Change it to the name of this notebook!\n",
        "!jupyter nbconvert --to PDF \"4740_FA20_p3_im324_kl866.ipynb\""
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NbConvertApp] Converting notebook 4740_FA20_p3_im324_kl866.ipynb to PDF\n",
            "[NbConvertApp] Writing 181635 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 165666 bytes to 4740_FA20_p3_im324_kl866.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}